{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape SATP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jan 2000\n",
      "Feb 2000\n",
      "Mar 2000\n",
      "Apr 2000\n",
      "May 2000\n",
      "Jun 2000\n",
      "Jul 2000\n",
      "Aug 2000\n",
      "Sep 2000\n",
      "Oct 2000\n",
      "Nov 2000\n",
      "Dec 2000\n",
      "Jan 2001\n",
      "Feb 2001\n",
      "Mar 2001\n",
      "Apr 2001\n",
      "May 2001\n",
      "Jun 2001\n",
      "Jul 2001\n",
      "Aug 2001\n",
      "Sep 2001\n",
      "Oct 2001\n",
      "Nov 2001\n",
      "Dec 2001\n",
      "Jan 2002\n",
      "Feb 2002\n",
      "Mar 2002\n",
      "Apr 2002\n",
      "May 2002\n",
      "Jun 2002\n",
      "Jul 2002\n",
      "Aug 2002\n",
      "Sep 2002\n",
      "Oct 2002\n",
      "Nov 2002\n",
      "Dec 2002\n",
      "Jan 2003\n",
      "Feb 2003\n",
      "Mar 2003\n",
      "Apr 2003\n",
      "May 2003\n",
      "Jun 2003\n",
      "Jul 2003\n",
      "Aug 2003\n",
      "Sep 2003\n",
      "Oct 2003\n",
      "Nov 2003\n",
      "Dec 2003\n",
      "Jan 2004\n",
      "Feb 2004\n",
      "Mar 2004\n",
      "Apr 2004\n",
      "May 2004\n",
      "Jun 2004\n",
      "Jul 2004\n",
      "Aug 2004\n",
      "Sep 2004\n",
      "Oct 2004\n",
      "Nov 2004\n",
      "Dec 2004\n",
      "Jan 2005\n",
      "Feb 2005\n",
      "Mar 2005\n",
      "Apr 2005\n",
      "May 2005\n",
      "Jun 2005\n",
      "Jul 2005\n",
      "Aug 2005\n",
      "Sep 2005\n",
      "Oct 2005\n",
      "Nov 2005\n",
      "Dec 2005\n",
      "Jan 2006\n",
      "Feb 2006\n",
      "Mar 2006\n",
      "Apr 2006\n",
      "May 2006\n",
      "Jun 2006\n",
      "Jul 2006\n",
      "Aug 2006\n",
      "Sep 2006\n",
      "Oct 2006\n",
      "Nov 2006\n",
      "Dec 2006\n",
      "Jan 2007\n",
      "Feb 2007\n",
      "Mar 2007\n",
      "Apr 2007\n",
      "May 2007\n",
      "Jun 2007\n",
      "Jul 2007\n",
      "Aug 2007\n",
      "Sep 2007\n",
      "Oct 2007\n",
      "Nov 2007\n",
      "Dec 2007\n",
      "Jan 2008\n",
      "Feb 2008\n",
      "Mar 2008\n",
      "Apr 2008\n",
      "May 2008\n",
      "Jun 2008\n",
      "Jul 2008\n",
      "Aug 2008\n",
      "Sep 2008\n",
      "Oct 2008\n",
      "Nov 2008\n",
      "Dec 2008\n",
      "Jan 2009\n",
      "Feb 2009\n",
      "Mar 2009\n",
      "Apr 2009\n",
      "May 2009\n",
      "Jun 2009\n",
      "Jul 2009\n",
      "Aug 2009\n",
      "Sep 2009\n",
      "Oct 2009\n",
      "Nov 2009\n",
      "Dec 2009\n",
      "Jan 2010\n",
      "Feb 2010\n",
      "Mar 2010\n",
      "Apr 2010\n",
      "May 2010\n",
      "Jun 2010\n",
      "Jul 2010\n",
      "Aug 2010\n",
      "Sep 2010\n",
      "Oct 2010\n",
      "Nov 2010\n",
      "Dec 2010\n",
      "Jan 2011\n",
      "Feb 2011\n",
      "Mar 2011\n",
      "Apr 2011\n",
      "May 2011\n",
      "Jun 2011\n",
      "Jul 2011\n",
      "Aug 2011\n",
      "Sep 2011\n",
      "Oct 2011\n",
      "Nov 2011\n",
      "Dec 2011\n",
      "Jan 2012\n",
      "Feb 2012\n",
      "Mar 2012\n",
      "Apr 2012\n",
      "May 2012\n",
      "Jun 2012\n",
      "Jul 2012\n",
      "Aug 2012\n",
      "Sep 2012\n",
      "Oct 2012\n",
      "Nov 2012\n",
      "Dec 2012\n",
      "Jan 2013\n",
      "Feb 2013\n",
      "Mar 2013\n",
      "Apr 2013\n",
      "May 2013\n",
      "Jun 2013\n",
      "Jul 2013\n",
      "Aug 2013\n",
      "Sep 2013\n",
      "Oct 2013\n",
      "Nov 2013\n",
      "Dec 2013\n",
      "Jan 2014\n",
      "Feb 2014\n",
      "Mar 2014\n",
      "Apr 2014\n",
      "May 2014\n",
      "Jun 2014\n",
      "Jul 2014\n",
      "Aug 2014\n",
      "Sep 2014\n",
      "Oct 2014\n",
      "Nov 2014\n",
      "Dec 2014\n",
      "Jan 2015\n",
      "Feb 2015\n",
      "Mar 2015\n",
      "Apr 2015\n",
      "May 2015\n",
      "Jun 2015\n",
      "Jul 2015\n",
      "Aug 2015\n",
      "Sep 2015\n",
      "Oct 2015\n",
      "Nov 2015\n",
      "Dec 2015\n",
      "Jan 2016\n",
      "Feb 2016\n",
      "Mar 2016\n",
      "Apr 2016\n",
      "May 2016\n",
      "Jun 2016\n",
      "Jul 2016\n",
      "Aug 2016\n",
      "Sep 2016\n",
      "Oct 2016\n",
      "Nov 2016\n",
      "Dec 2016\n",
      "Jan 2017\n",
      "Feb 2017\n",
      "Mar 2017\n",
      "Apr 2017\n",
      "May 2017\n",
      "Jun 2017\n",
      "Jul 2017\n",
      "Aug 2017\n",
      "Sep 2017\n",
      "Oct 2017\n",
      "Nov 2017\n",
      "Dec 2017\n",
      "Jan 2018\n",
      "Feb 2018\n",
      "Mar 2018\n",
      "Apr 2018\n",
      "May 2018\n",
      "Jun 2018\n",
      "Jul 2018\n",
      "Aug 2018\n",
      "Sep 2018\n",
      "Oct 2018\n",
      "Nov 2018\n",
      "Dec 2018\n",
      "Jan 2019\n",
      "Feb 2019\n",
      "Mar 2019\n",
      "Apr 2019\n",
      "May 2019\n",
      "Jun 2019\n",
      "Jul 2019\n",
      "Aug 2019\n",
      "Sep 2019\n",
      "Oct 2019\n",
      "Nov 2019\n",
      "Dec 2019\n",
      "Jan 2020\n",
      "Feb 2020\n",
      "Mar 2020\n",
      "Apr 2020\n",
      "May 2020\n",
      "Jun 2020\n",
      "Jul 2020\n",
      "Aug 2020\n",
      "Sep 2020\n",
      "Oct 2020\n",
      "Nov 2020\n",
      "Dec 2020\n",
      "Jan 2021\n",
      "Feb 2021\n",
      "Mar 2021\n",
      "Apr 2021\n",
      "May 2021\n",
      "Jun 2021\n",
      "Jul 2021\n",
      "Aug 2021\n",
      "Sep 2021\n",
      "Oct 2021\n",
      "Nov 2021\n",
      "Dec 2021\n",
      "Jan 2022\n",
      "Feb 2022\n",
      "Mar 2022\n",
      "Apr 2022\n",
      "May 2022\n",
      "Jun 2022\n",
      "Jul 2022\n",
      "Aug 2022\n",
      "Sep 2022\n",
      "Oct 2022\n",
      "Nov 2022\n",
      "Dec 2022\n",
      "Jan 2023\n",
      "Feb 2023\n",
      "Mar 2023\n",
      "Apr 2023\n",
      "May 2023\n",
      "Jun 2023\n",
      "Jul 2023\n",
      "Aug 2023\n",
      "Sep 2023\n",
      "Oct 2023\n",
      "Nov 2023\n",
      "Dec 2023\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Set up\n",
    "months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "years = list(range(2001, 2023 + 1))\n",
    "letters_to_delete = [\"“\", \"”\", \"‘\", \"’\", \"'\", \"\\nRead less...\"]\n",
    "pattern = \"|\".join(map(re.escape, letters_to_delete))\n",
    "\n",
    "def download_data(month, year):\n",
    "    # Generate a dataset using the string\n",
    "    theurl = f\"https://www.satp.org/terrorist-activity/india-{month}-{year}\"\n",
    "    response = requests.get(theurl)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table')  # Assuming the table of interest is the first one\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "    \n",
    "    data = []\n",
    "    for row in rows:\n",
    "        date_cell = row.find('td', style=\"width: 15%;\")\n",
    "        if not date_cell:\n",
    "            continue\n",
    "        date = date_cell.get_text(strip=True)\n",
    "        \n",
    "        description_cell = row.find('td', colspan=\"2\", style=\"width: 85%; font-family: Arial,Helvetica,sans-serif; font-size: small; text-align: justify; color: #294A6F;\")\n",
    "        if not description_cell:\n",
    "            continue\n",
    "        description_div = description_cell.find('div', class_=\"more\")\n",
    "        description = description_div.get_text(strip=True) if description_div else \"\"\n",
    "        \n",
    "        data.append([date, description, year])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=[\"date\", \"description\", \"year\"])\n",
    "    \n",
    "    df[\"date\"] = df[\"date\"].str.replace(\"&nbsp\", \"\", regex=False)\n",
    "    df[\"date\"] = df[\"date\"].str.replace(\" - \", \" \", regex=False)\n",
    "    df[\"description\"] = df[\"description\"].str.replace(pattern, \"\", regex=True)\n",
    "    print(month,year)\n",
    "    return df\n",
    "\n",
    "# Download data from year 2000\n",
    "year = 2000\n",
    "datasets = [download_data(month, year) for month in months]\n",
    "dataset_satp = pd.concat(datasets, ignore_index=True)\n",
    "dataset_satp.columns = [\"date\", \"description\", \"year\"]\n",
    "\n",
    "satp_new = dataset_satp.copy()\n",
    "\n",
    "# Download data for all other years and append to 2000 data\n",
    "for year in years:\n",
    "    datasets = [download_data(month, year) for month in months]\n",
    "    dataset_year = pd.concat(datasets, ignore_index=True)\n",
    "    satp_new = pd.concat([satp_new, dataset_year], ignore_index=True)\n",
    "\n",
    "satp_new[\"description\"] = satp_new[\"description\"].str.replace(\"Read less...\", \"\", regex=False)\n",
    "satp_new.to_csv(\"/Users/jonathanold/Library/CloudStorage/GoogleDrive-jonathan_old@berkeley.edu/My Drive/_Berkeley Research/Reservations and Conflict/Data/_gen/satp_scraped_python.csv\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satp_new.reset_index(inplace=True)\n",
    "satp_new['id_satp_new'] = 'id_'+ satp_new['index'].astype(str)\n",
    "\n",
    "satp_new.to_csv(\"/Users/jonathanold/Library/CloudStorage/GoogleDrive-jonathan_old@berkeley.edu/My Drive/_Berkeley Research/Reservations and Conflict/Data/_gen/satp_scraped_python.csv\", encoding='utf-8',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in data\n",
      "Starting merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/pz74ttxj0g9ggdqz9nnmm5q00000gn/T/ipykernel_3484/1330238046.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_1b['merge1'] = 1\n",
      "/var/folders/yz/pz74ttxj0g9ggdqz9nnmm5q00000gn/T/ipykernel_3484/1330238046.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_2b['merge2'] = 1\n",
      "/var/folders/yz/pz74ttxj0g9ggdqz9nnmm5q00000gn/T/ipykernel_3484/1330238046.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_df_filtered['merge3'] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118267\n",
      "118322\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "satp_new = pd.read_csv(\"/Users/jonathanold/Library/CloudStorage/GoogleDrive-jonathan_old@berkeley.edu/My Drive/_Berkeley Research/Reservations and Conflict/Data/_gen/satp_scraped_python.csv\")\n",
    "satp_new_orig = satp_new\n",
    "satp_new = satp_new.iloc[:118399]\n",
    "\n",
    "satp_new = satp_new.drop_duplicates(subset=['description','year','date'])\n",
    "satp_new = satp_new.dropna(subset=['description'])\n",
    "satp_new['d_new'] = satp_new['description'].str.strip()\n",
    "satp_new = satp_new[['d_new','id_satp_new']]\n",
    "satp_old = pd.read_csv(\"/Users/jonathanold/Library/CloudStorage/GoogleDrive-jonathan_old@berkeley.edu/My Drive/_Berkeley Research/Reservations and Conflict/Data/_gen/coding_events/data_sample_new.csv\")\n",
    "satp_old_orig = satp_old\n",
    "satp_old = satp_old.drop_duplicates(subset=['description','year'])\n",
    "satp_old = satp_old.dropna(subset=['description'])\n",
    "satp_old['d_old'] = satp_old['description'].str.strip()\n",
    "satp_old['id_satp_old'] = satp_old['id_satp'] \n",
    "satp_old = satp_old[['d_old','id_satp_old']]\n",
    "\n",
    "# Define a function to transform the description\n",
    "def transform_description(desc):\n",
    "    desc = desc.lower()  # Convert to lower case\n",
    "    desc = re.sub(r'[^a-z0-9\\s]', '', desc)  # Remove special characters and punctuation\n",
    "    return desc\n",
    "\n",
    "# Apply the transformation to the description column\n",
    "satp_old['dm_old'] = satp_old['d_old'].apply(transform_description)\n",
    "satp_new['dm_new'] = satp_new['d_new'].apply(transform_description)\n",
    "\n",
    "\n",
    "print(\"Read in data\")\n",
    "satp_old = satp_old[::-1]\n",
    "print(\"Starting merge\")\n",
    "\n",
    "\n",
    "\n",
    "##### Do the actual merge\n",
    "# 1. Merge based on d_new and d_old\n",
    "merged_1 = pd.merge(satp_old, satp_new, left_on='d_old', right_on='d_new', how='outer', indicator=True, suffixes=('_old', '_new'))\n",
    "\n",
    "# 2. Look at observations from satp_old that were NOT merged, and observations from satp_new that were NOT merged\n",
    "unmerged_old_1 = merged_1[merged_1['_merge'] == 'left_only']\n",
    "unmerged_new_1 = merged_1[merged_1['_merge'] == 'right_only']\n",
    "\n",
    "merged_1b = merged_1[merged_1['_merge'] == 'both']\n",
    "merged_1b['merge1'] = 1\n",
    "\n",
    "# Create new DataFrames for second merge\n",
    "unmerged_old_1 = unmerged_old_1[['id_satp_old', 'd_old', 'dm_old']]\n",
    "unmerged_new_1 = unmerged_new_1[['id_satp_new', 'd_new', 'dm_new']]\n",
    "\n",
    "# 3. Merge these based on dm_new and dm_old\n",
    "merged_2 = pd.merge(unmerged_old_1, unmerged_new_1, left_on='dm_old', right_on='dm_new', how='outer', indicator=True, suffixes=('_old', '_new'))\n",
    "merged_2b = merged_2[merged_2['_merge'] == 'both']\n",
    "merged_2b['merge2'] = 1\n",
    "# 4. Append to df_merge_final, which contains both merges\n",
    "df_merge_final = pd.concat([merged_1b, merged_2b], ignore_index=True)\n",
    "\n",
    "# Create DataFrames of unmerged observations from the second merge\n",
    "unmerged_old_2 = merged_2[merged_2['_merge'] == 'left_only']\n",
    "unmerged_new_2 = merged_2[merged_2['_merge'] == 'right_only']\n",
    "\n",
    "# Drop the '_merge' column\n",
    "df_merge_final.drop(columns=['_merge'], inplace=True)\n",
    "unmerged_old_2 = unmerged_old_2.drop(columns=['_merge','d_new','dm_new'])\n",
    "unmerged_new_2 = unmerged_new_2.drop(columns=['_merge','d_old','dm_old'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "\n",
    "# Function to perform fuzzy matching\n",
    "def fuzzy_merge(df_1, df_2, key1, key2, threshold=90):\n",
    "    \"\"\"\n",
    "    df_1 is the left table to join\n",
    "    df_2 is the right table to join\n",
    "    key1 is the key column of the left table\n",
    "    key2 is the key column of the right table\n",
    "    threshold is how close the matches should be to return a match, based on Levenshtein distance\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    scores = []\n",
    "    # Iterate over the entries in df_1[key1]\n",
    "    for value in df_1[key1]:\n",
    "        # Skip matching if the value is null\n",
    "        if pd.isnull(value):\n",
    "            matches.append(None)\n",
    "            scores.append(None)\n",
    "        else:\n",
    "            match, score, i = process.extractOne(value, df_2[key2], scorer=fuzz.token_sort_ratio)\n",
    "            if match is not None:\n",
    "                matches.append(match)\n",
    "                scores.append(score)\n",
    "            else:\n",
    "                matches.append(None)\n",
    "                scores.append(None)\n",
    "\n",
    "    \n",
    "    df_1['best_match'] = matches\n",
    "    df_1['score'] = scores\n",
    "    \n",
    "    # Filter matches based on the threshold\n",
    "    df_1_filtered = df_1[df_1['score'] >= threshold]\n",
    "    \n",
    "    # Perform the actual merge\n",
    "    df_2.set_index(key2, inplace=True)\n",
    "    merged_df = df_1_filtered.merge(df_2, left_on='best_match', right_index=True, how='outer', indicator=True)\n",
    "    unmerged_old_3 = merged_df[merged_df['_merge'] == 'left_only']\n",
    "    unmerged_new_3 = merged_df[merged_df['_merge'] == 'right_only']\n",
    "    merged_df_filtered = merged_df[merged_df['_merge'] == 'both']\n",
    "    return merged_df, unmerged_old_3, unmerged_new_3, merged_df_filtered\n",
    "\n",
    "# Apply the fuzzy_merge function\n",
    "fuzzy_merged_df, unmerged_old_3, unmerged_new_3, merged_df_filtered = fuzzy_merge(unmerged_old_2, unmerged_new_2, 'dm_old', 'dm_new', threshold=90)\n",
    "merged_df_filtered['merge3'] = 1\n",
    "\n",
    "\n",
    "df_merge_final = pd.concat([df_merge_final, merged_df_filtered], ignore_index=True)\n",
    "df_merge_final['id_satp_old'] = df_merge_final['id_satp_old'].fillna(df_merge_final['id_satp_old_x'])\n",
    "df_merge_final['id_satp_new'] = df_merge_final['id_satp_new'].fillna(df_merge_final['id_satp_new_y'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(df_merge_final['id_satp_old'].nunique())\n",
    "print(df_merge_final['id_satp_new'].nunique())\n",
    "df_merge_final['is_duplicate_new'] = df_merge_final.duplicated(subset='id_satp_new', keep=False)\n",
    "df_merge_final['is_duplicate_old'] = df_merge_final.duplicated(subset='id_satp_old', keep=False)\n",
    "\n",
    "# Filter: Drop duplicates (either side)\n",
    "df_merge_new = df_merge_final.drop_duplicates(subset='id_satp_new', keep='first')\n",
    "df_merge_old = df_merge_final.drop_duplicates(subset='id_satp_old', keep='first')\n",
    "\n",
    "df_merge_new = df_merge_new[['id_satp_new','id_satp_old']]\n",
    "df_merge_old = df_merge_old[['id_satp_new','id_satp_old']]\n",
    "\n",
    "# Make final \"new\" dataset \n",
    "df_new = pd.merge(satp_new_orig, df_merge_new, on=\"id_satp_new\", how=\"left\", indicator=True)\n",
    "df_orig_only = df_new[df_new['_merge'] == 'left_only']\n",
    "df_new_only = df_new[df_new['_merge'] == 'right_only']\n",
    "df_both_only = df_new[df_new['_merge'] == 'both']\n",
    "\n",
    "df_new = df_new[['date','year','description','id_satp_old','id_satp_new']]\n",
    "# Make final \"old\" dataset \n",
    "df_old = pd.merge(satp_old_orig, df_merge_old, left_on=\"id_satp\", right_on=\"id_satp_old\", how=\"outer\", indicator=True)\n",
    "df_orig_only = df_old[df_old['_merge'] == 'left_only']\n",
    "df_new_only = df_old[df_old['_merge'] == 'right_only']\n",
    "df_both_only = df_old[df_old['_merge'] == 'both']\n",
    "\n",
    "\n",
    "# Merge Final dataset with training data\n",
    "df_training = pd.read_csv(\"/Users/jonathanold/Library/CloudStorage/GoogleDrive-jonathan_old@berkeley.edu/My Drive/_Berkeley Research/Reservations and Conflict/Data/_gen/coding_events/sample_events_coded_new.csv\")\n",
    "df_training = df_training.iloc[:, 3:]\n",
    "\n",
    "variables_list = ['relevant_event', 'multiple_events','internal_conflict']  # replace with your actual variable names\n",
    "# Keep only the variables which are non-missing in any of the variables in variables_list\n",
    "df_training= df_training.dropna(subset=variables_list, how='all')\n",
    "\n",
    "df_training_final = pd.merge(df_new, df_training, left_on='id_satp_old', right_on='id_satp',how='left')\n",
    "\n",
    "\n",
    "df_training_final.to_csv(\"/Users/jonathanold/Library/CloudStorage/GoogleDrive-jonathan_old@berkeley.edu/My Drive/_Berkeley Research/Reservations and Conflict/Data/_gen/coding_events/satp_training_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_merge_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_merge_final\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_satp_old\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_merge_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_satp_new\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique())\n\u001b[1;32m      3\u001b[0m df_merge_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_duplicate_new\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_merge_final\u001b[38;5;241m.\u001b[39mduplicated(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_satp_new\u001b[39m\u001b[38;5;124m'\u001b[39m, keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_merge_final' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rapidfuzz \n",
    "filtered_df = df_merge_final[(df_merge_final['is_duplicate_old'] == True) | (df_merge_final['is_duplicate_new'] == True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting\")\n",
    "inspect_merge = pd.merge(satp_new, final_merged, left_on='id_satp_new', right_on='id_satp_new', how='left', indicator=True)\n",
    "# Filter out rows that are only in df2 (right_only)\n",
    "df2_not_in_df1 = inspect_merge[inspect_merge['_merge'] == 'right_only']\n",
    "\n",
    "\n",
    "inspect_merge2 = pd.merge(satp_old, final_merged, left_on='id_satp', right_on='id_satp',  how='left', indicator=True)\n",
    "# Filter out rows that are only in df2 (right_only)\n",
    "old_not_in_df1 = inspect_merge2[inspect_merge2['_merge'] == 'left_only']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_df1 = satp_new[satp_new.duplicated(subset=['description'], keep=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_satp_old', 'one_old', 'id_satp_new', 'one_new', 'd_new'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmerged_new_2.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
